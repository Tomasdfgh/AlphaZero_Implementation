#!/usr/bin/env python3
"""
Neural Network Training Script for Aldarion Chess Engine

This script trains the ChessNet model on self-play data generated by parallel_training_data.py.
Data format: List of (board_fen, history_fens, move_probabilities_dict, game_outcome) tuples

I also reduced the value loss by 50 percent. That can be seen in compute_loss function. The issue
is that since most games are still random at the beginning, the value loss is overfitting right away;
however I added a couple of experiments to see the accuracy of how the model predicts value: one where
the overall accuracy of the value is recorded and one where only the values of the decisive games are recorded.
And both went up over the epochs while the validation value loss is still increasing.

This is my hypothesis for why the value is overfitting. Moves are assigned a value for training based on whether
or not the team that assigned that move won the game. So if white won, all the moves that white made in that game
gets assigned a 1, and all of black gets a -1 and vice versa. This could be problematic for games at the beginning
when they tend to be more random. Because of that, there will be a lot of similar data points with the same board
position and side to move that has opposing z (z is for value). That means the model will have a hard time learning
how to classify it properly.

Two things are clear. First is that the two metric that I test on the validation data is indeed improving (which is
interesting because why is the validation loss still going up then? I haven't thought about it too much but im sure
theres a logical explaination for it). The second is that the resulting model is indeed beating out old model before
it, so that means it is "better". As we continue to train though, I suspect that the value loss will eventually stop
overfitting immediately. This is because the moves will no longer be random and there will be proper patterns that
could show things like "this opening move is shit".
"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pickle
import argparse
import chess
from datetime import datetime
import matplotlib.pyplot as plt

# Import existing modules
import model as md
import board_reader as br

class ChessTrainingDataset(Dataset):
    """
    PyTorch Dataset for chess training data with fixed 4,672-move vocabulary
    """
    def __init__(self, data_files):
        
        all_training_data = []
        for data_file in data_files:
            if not os.path.exists(data_file):
                print(f"Error: Could not find {data_file}")
                continue

            try:
                with open(data_file, 'rb') as f:
                    data = pickle.load(f)
                all_training_data.extend(data)
            except Exception as e:
                print(f"Error loading {data_file}: {e}")
                continue
        
        self.training_data = all_training_data
    
    def __len__(self):
        return len(self.training_data)
    
    def __getitem__(self, idx):
        board_fen, history_fens, move_probs, game_outcome = self.training_data[idx]
        
        game_history = []
        for fen in history_fens:
            game_history.append(chess.Board(fen))
        
        current_board = chess.Board(board_fen)
        board_tensor = br.board_to_full_alphazero_input(current_board, game_history)

        policy_vector = torch.zeros((8, 8, 73), dtype=torch.float32)
        for move, prob in move_probs.items():
            try:
                r, c, pl = br.uci_to_policy_index(str(move), current_board.turn)
                policy_vector[r, c, pl] = float(prob)
            except:
                continue
        policy_vector = policy_vector.reshape(-1)
        
        s = policy_vector.sum()
        if s > 0:
            policy_vector /= s
        
        legal_mask = br.create_legal_move_mask(chess.Board(board_fen)).flatten()
        if not legal_mask.any():
            return None
        value_target = torch.tensor([game_outcome], dtype=torch.float32)
        
        return board_tensor.float(), policy_vector, legal_mask, value_target

def collate_fn(batch):
    batch = [item for item in batch if item is not None]
    if len(batch) == 0:
        return torch.empty(0), torch.empty(0), torch.empty(0), torch.empty(0)
    
    return torch.utils.data.dataloader.default_collate(batch)


def compute_loss(model_output, targets, legal_masks):
    """
    Compute AlphaZero loss with proper legal move masking.
    Heads up that L2 Regularization is not done here but instead done
    through the optimizer
    """

    policy_logits, value_pred = model_output
    target_policy, target_value = targets
    

    masked_logits = policy_logits.clone()
    masked_logits[~legal_masks.bool()] = -1000.0
    log_probs = F.log_softmax(masked_logits, dim=1)
    
    target_sum = target_policy.sum(dim=1, keepdim=True)
    normalized_target = target_policy / (target_sum + 1e-8)
    
    # Policy Loss: - pi * log (p)
    policy_loss = -(normalized_target * log_probs).sum(dim=1).mean()
    
    # Value loss: (v - t) ** 2
    value_loss = nn.MSELoss()(value_pred.squeeze(), target_value.squeeze())
    
    # Total Loss: (v - t) ** 2 - pi * log(p)
    total_loss = policy_loss +  0.5 * value_loss
    
    return {
        'total_loss': total_loss,
        'policy_loss': policy_loss,
        'value_loss': value_loss
    }


def train_epoch(model, dataloader, optimizer, device, epoch_num):
    """
    Train model for one epoch
    """
    model.train()
    
    total_loss = 0.0
    total_policy_loss = 0.0
    total_value_loss = 0.0
    num_batches = 0
    
    for batch_idx, (board_tensors, target_policies, legal_masks, target_values) in enumerate(dataloader):

        if len(board_tensors) == 0:
            continue
            
        board_tensors = board_tensors.to(device)
        target_policies = target_policies.to(device)
        legal_masks = legal_masks.to(device)
        target_values = target_values.to(device)
        
        optimizer.zero_grad()
        policy_logits, value_pred = model(board_tensors)
        
        losses = compute_loss(
            (policy_logits, value_pred), 
            (target_policies, target_values),
            legal_masks
        )
        
        losses['total_loss'].backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)
        
        optimizer.step()
        
        total_loss += losses['total_loss'].item()
        total_policy_loss += losses['policy_loss'].item()
        total_value_loss += losses['value_loss'].item()
        num_batches += 1
        
        if batch_idx % 100 == 0:
            print(f"Epoch {epoch_num}, Batch {batch_idx}/{len(dataloader)}: "
                  f"Loss={losses['total_loss'].item():.4f}, "
                  f"Policy={losses['policy_loss'].item():.4f}, "
                  f"Value={losses['value_loss'].item():.4f}")
    
    metrics = {
        'avg_total_loss': total_loss / num_batches,
        'avg_policy_loss': total_policy_loss / num_batches,
        'avg_value_loss': total_value_loss / num_batches
    }
    
    return metrics


def validate_model(model, dataloader, device):
    """
    Validate model on held-out data
    """
    model.eval()
    
    total_loss = 0.0
    total_policy_loss = 0.0
    total_value_loss = 0.0
    num_batches = 0
    
    # EXPERIMENT: Sign accuracy P[sign(v)==sign(z) or z==0]
    total_sign_accuracy = 0.0
    total_sign_accuracy_decisive = 0.0
    total_decisive_samples = 0
    total_draw_samples = 0
    
    with torch.no_grad():

        for board_tensors, target_policies, legal_masks, target_values in dataloader:

            if len(board_tensors) == 0:
                continue
                
            board_tensors = board_tensors.to(device)
            target_policies = target_policies.to(device)
            legal_masks = legal_masks.to(device)
            target_values = target_values.to(device)

            policy_logits, value_pred = model(board_tensors)
            
            losses = compute_loss(
                (policy_logits, value_pred), 
                (target_policies, target_values),
                legal_masks
            )
            
            total_loss += losses['total_loss'].item()
            total_policy_loss += losses['policy_loss'].item()
            total_value_loss += losses['value_loss'].item()
            
            # EXPERIMENT: Sign accuracy P[sign(v)==sign(z) or z==0]
            correct_sign_or_draw = ((torch.sign(value_pred.squeeze()) == torch.sign(target_values.squeeze())) | (target_values.squeeze() == 0)).float()
            sign_accuracy = correct_sign_or_draw.mean()
            total_sign_accuracy += sign_accuracy.item()
            
            # EXPERIMENT: Track decisive games only (|z| = 1)
            decisive_mask = torch.abs(target_values.squeeze()) == 1
            if decisive_mask.any():
                decisive_correct = (torch.sign(value_pred.squeeze()[decisive_mask]) == 
                                  torch.sign(target_values.squeeze()[decisive_mask])).float()
                total_sign_accuracy_decisive += decisive_correct.sum().item()
                total_decisive_samples += decisive_mask.sum().item()
            
            # Track draw samples
            draw_mask = target_values.squeeze() == 0
            total_draw_samples += draw_mask.sum().item()
            
            num_batches += 1
    
    # Calculate dataset composition and baselines
    total_samples = total_decisive_samples + total_draw_samples
    draw_ratio = total_draw_samples / total_samples if total_samples > 0 else 0
    decisive_ratio = total_decisive_samples / total_samples if total_samples > 0 else 0
    trivial_baseline = draw_ratio  # "Always predict draw" baseline
    
    metrics = {
        'avg_total_loss': total_loss / num_batches,
        'avg_policy_loss': total_policy_loss / num_batches,
        'avg_value_loss': total_value_loss / num_batches,
        
        #----------------All Metrics Below are only experimental so it doesnt impact grad backprop------------------#
        'sign_accuracy': total_sign_accuracy / num_batches,
        'sign_accuracy_decisive': total_sign_accuracy_decisive / total_decisive_samples if total_decisive_samples > 0 else 0,
        'draw_ratio': draw_ratio,
        'decisive_ratio': decisive_ratio,
        'trivial_baseline': trivial_baseline 
    }
    
    return metrics

def create_alphazero_lr_scheduler(optimizer, total_epochs):
    """
    Adaptation of AlphaZero learning rate scheduler. They did a bit differently than me
    """
    def lr_lambda(epoch):
        decay_epoch1 = int(0.4 * total_epochs)
        decay_epoch2 = int(0.6 * total_epochs)
        
        if epoch < decay_epoch1:
            return 1.0
        elif epoch < decay_epoch2:
            return 0.1
        else:
            return 0.01
    
    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

def plot_training_metrics(train_metrics_history, val_metrics_history, save_path=None):

    epochs = range(1, len(train_metrics_history) + 1)
    
    _, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # Total loss
    axes[0].plot(epochs, [m['avg_total_loss'] for m in train_metrics_history], 'b-', label='Train')
    if val_metrics_history:
        axes[0].plot(epochs, [m['avg_total_loss'] for m in val_metrics_history], 'r-', label='Validation')
    axes[0].set_title('Total Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True)
    
    # Policy loss
    axes[1].plot(epochs, [m['avg_policy_loss'] for m in train_metrics_history], 'b-', label='Train')
    if val_metrics_history:
        axes[1].plot(epochs, [m['avg_policy_loss'] for m in val_metrics_history], 'r-', label='Validation')
    axes[1].set_title('Policy Loss')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss')
    axes[1].legend()
    axes[1].grid(True)
    
    # Value loss
    axes[2].plot(epochs, [m['avg_value_loss'] for m in train_metrics_history], 'b-', label='Train')
    if val_metrics_history:
        axes[2].plot(epochs, [m['avg_value_loss'] for m in val_metrics_history], 'r-', label='Validation')
    axes[2].set_title('Value Loss')
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('Loss')
    axes[2].legend()
    axes[2].grid(True)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Training curves saved to {save_path}")
    
    plt.show()


def main():
    parser = argparse.ArgumentParser(
        description='Train ChessNet on self-play data',
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    parser.add_argument('--data', type=str, nargs='+', required=True)
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch_size', type=int, default=1024)
    parser.add_argument('--lr', type=float, default=0.2)
    parser.add_argument('--weight_decay', type=float, default=1e-4)
    parser.add_argument('--optimizer', type=str, default='sgd', choices=['sgd', 'adam'])
    parser.add_argument('--momentum', type=float, default=0.9)
    parser.add_argument('--lr_schedule', type=str, default='alphazero', choices=['alphazero', 'step', 'none'])
    parser.add_argument('--validation_data', type=str, nargs='+', default=None)
    parser.add_argument('--model_path', type=str, default='model_weights/model_weights.pth')
    parser.add_argument('--output', type=str, default=None)
    
    args = parser.parse_args()
    
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    print("="*70)
    print("Initial Training Setup")
    print("="*70)
    print('\n')
    
    print(f"Using device: {device}")
    print(f"Optimizer: {args.optimizer.upper()}")
    if args.optimizer == 'sgd':
        print(f"Momentum: {args.momentum}")
    print(f"Learning rate: {args.lr}")
    print(f"LR Schedule: {args.lr_schedule}")
    print(f"Validation: {'Separate dataset' if args.validation_data else 'Disabled'}")
    
    # Create datasets
    train_dataset = ChessTrainingDataset(args.data)
    
    if args.validation_data:
        val_dataset = ChessTrainingDataset(args.validation_data)
        print(f"Train set: {len(train_dataset)}, Validation set: {len(val_dataset)}")
    else:
        val_dataset = None
        print(f"Train set: {len(train_dataset)}, No validation data")
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=args.batch_size, 
        shuffle=True,
        num_workers=4,
        pin_memory=True if device.type == 'cuda' else False,
        collate_fn=collate_fn
    )
    
    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=args.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True if device.type == 'cuda' else False,
            collate_fn=collate_fn
        )
    
    # Initialize model
    model = md.ChessNet()
    model = model.to(device)
    
    # Load pretrained weights if available
    if os.path.exists(args.model_path):
        print(f"\nModel weights from {args.model_path}")
        try:
            state_dict = torch.load(args.model_path, map_location=device, weights_only=True)
            model.load_state_dict(state_dict)
            print("Pretrained weights loaded successfully")
        except Exception as e:
            print(f"Could not load pretrained weights: {e}, so starting with random weights")
    else:
        print("No pretrained weights found. Starting with random initialization")
    
    # Setup optimizer and learning rate scheduler
    if args.optimizer == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)
        print(f"Using SGD optimizer (AlphaZero paper): lr={args.lr}, momentum={args.momentum}")
    else:
        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
        print(f"Using Adam optimizer: lr={args.lr}")
    
    # Setup learning rate scheduler
    if args.lr_schedule == 'alphazero':
        scheduler = create_alphazero_lr_scheduler(optimizer, args.epochs)
        print(f"Using AlphaZero learning rate schedule (decay at epochs {int(0.4 * args.epochs)} and {int(0.6 * args.epochs)})")
    elif args.lr_schedule == 'step':
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)
        print("Using step LR schedule (decay every 10 epochs)")
    else:
        scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 1.0)
        print("Using constant learning rate (no schedule)")
    
    # Training loop
    print(f"\nStarting training for {args.epochs} epochs...")
    print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")
    print(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
    
    train_metrics_history = []
    val_metrics_history = []
    best_val_loss = float('inf')
    patience_counter = 0
    patience = 3
    min_improvement = 0.0025  # 0.25% minimum improvement threshold

    print('\n')
    print("="*70)
    print("Training Begins")
    print("="*70)
    print('\n')
    
    for epoch in range(1, args.epochs + 1):
        print(f"Epoch {epoch}/{args.epochs}")
        print("-" * 50)
        
        # Training
        train_metrics = train_epoch(model, train_loader, optimizer, device, epoch)
        train_metrics_history.append(train_metrics)
        
        print(f"Training - Loss: {train_metrics['avg_total_loss']:.4f}, "
              f"Policy: {train_metrics['avg_policy_loss']:.4f}, "
              f"Value: {train_metrics['avg_value_loss']:.4f}")
        
        # Validation
        if val_loader:
            val_metrics = validate_model(model, val_loader, device)
            val_metrics_history.append(val_metrics)
            
            print(f"Validation - Loss: {val_metrics['avg_total_loss']:.4f}, Policy: {val_metrics['avg_policy_loss']:.4f}, Value: {val_metrics['avg_value_loss']:.4f}")
            print('\n')
            print("Validation Experiments")
            print(f"Sign Accuracy Overall: {val_metrics['sign_accuracy']:.3f} (vs {val_metrics['trivial_baseline']:.3f} trivial baseline)")
            print(f"Sign Accuracy Decisive: {val_metrics['sign_accuracy_decisive']:.3f} (vs 0.500 random baseline)")
            print(f"Dataset: {val_metrics['draw_ratio']:.1%} draws, {val_metrics['decisive_ratio']:.1%} decisive")
            
            # Early Stopping with relative improvement threshold
            current_val_loss = val_metrics['avg_total_loss']
            
            if best_val_loss == float('inf'):
                # First epoch - set baseline
                best_val_loss = current_val_loss
                patience_counter = 0
                print(f"Baseline validation loss: {current_val_loss:.4f}\n")
            else:
                improvement = (best_val_loss - current_val_loss) / best_val_loss
                
                if improvement >= min_improvement:
                    best_val_loss = current_val_loss
                    patience_counter = 0
                    print(f"Validation loss improved by {improvement*100:.2f}%\n")
                else:
                    patience_counter += 1
                    if improvement > 0:
                        print(f"Validation loss improved by only {improvement*100:.2f}% (< {min_improvement*100:.2f}% threshold)\n")
                    else:
                        print(f"Validation loss increased by {abs(improvement)*100:.2f}%\n")
                    
                    if patience_counter >= patience:
                        print(f"Early stopping triggered: no improvement ≥{min_improvement*100:.2f}% for {patience} epochs\n")
                        break
        
        scheduler.step()
    
    # Set output directory for both model and plots
    if args.output is None:
        output_dir = "training_results"
        model_dir = "model_weights"
    else:
        output_dir = args.output
        model_dir = args.output
    
    # Save final model
    os.makedirs(model_dir, exist_ok=True)
    final_path = os.path.join(model_dir, "model_weights_final.pth")
    
    torch.save(model.state_dict(), final_path)
    print(f"\nFinal model saved to {final_path}")
    
    # Plot training curves
    if len(train_metrics_history) > 1:
        plot_dir = output_dir
        os.makedirs(plot_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_filename = f"training_curves_{timestamp}.png"
        plot_path = os.path.join(plot_dir, plot_filename)
        plot_training_metrics(train_metrics_history, val_metrics_history, plot_path)
    
    # Training summary
    print("\n" + "="*70)
    print("TRAINING COMPLETE")
    print("="*70)
    print(f"Final training loss: {train_metrics_history[-1]['avg_total_loss']:.4f}")
    if val_metrics_history:
        print(f"Final validation loss: {val_metrics_history[-1]['avg_total_loss']:.4f}")
    print(f"Total epochs: {args.epochs}")
    print(f"Final model: {final_path}")
    return final_path


if __name__ == "__main__":
    main()